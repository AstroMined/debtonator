***** SWITCH TO PLAN MODE!!! *****

Start session:
follow your custom instructions

Formulate a plan to continue on with the in-progress items, but keep the scope limited to work that can completed in a single session.
If changes are made to backend Python code, ensure the testing suite is adjusted to reflect those changes.
Ensure you modify fixtures, delete tests that are no longer relevant, and create or update tests to reflect new functionality.

When running the full test suite with pytest, run it like below to keep the output limited to conserve tokens in your context window:
pytest -q --tb=no --disable-warnings

If you need to run a specific single test to get more error context, you can do it with tracebacks if needed.
DO NOT RUN MORE THAN ONE SINGLE SPECIFIC TEST WITH TRACEBACKS.

I'll repeat it.
ONLY USE TRACEBACKS WHEN RUNNING A SINGLE SPECIFIC TEST.
For example:
pytest -v tests/unit/test_accounts.py::test_create_checking_account
You WILL lock up if you don't listen.

Also, there have been many significant code changes recently, but the test suite was NOT updated everywhere to reflect that.
Assume errors or failures in tests are due to a poorly written or a test that's no longer relevant before you assume there's a problem in the codebase.

End session:
update memory bank

Ensure you define a good starting point for our next working session.
Update README.md, CHANGELOG.md and update major, minor, or patch semantic versioning as needed.
Create ADRs ONLY if absolutely necessary to document a MAJOR architectural change.
Finally, suggest a good commit message for the tasks accomplished during this session.

follow your custom instructions

Formulate a plan to expand test coverage for models only, but keep the scope limited to work that can completed in a single session.
You'll first need to get a coverage report to see where the weaknesses are.
When running the full test suite with pytest, run it like below to keep the output limited to conserve tokens in your context window:
pytest -q --tb=no --disable-warnings

If you need to run a specific single test to get more error context, you can do it with tracebacks if needed.
DO NOT RUN MORE THAN ONE SINGLE SPECIFIC TEST WITH TRACEBACKS.

I'll repeat it.
ONLY USE TRACEBACKS WHEN RUNNING A SINGLE SPECIFIC TEST.
For example:
pytest -v tests/unit/test_accounts.py::test_create_checking_account
You will lock up if you don't listen.